---
title: 优化
tags: []
categories: [2025暑期班]
mathjax: true
math: true
notshow: true
date: 2025-07-21 08:56:48
---

## 优化

**核心问题**：我们通常需要解决一个无约束优化问题：

$minimize\ F(x)$

其中$x\in R^n$是我们要优化的目标向量（如顶点位置、相机参数、光照系数等），$F(x) : R^n -> R$ 是我们想要最小化的目标函数（如变形能量、渲染误差、物理势能等）。

<!--more-->

### 牛顿法(Newton's Method)

- **核心思想：** 利用目标函数 $F(x)$ 在当前点 $x_k$ 处的局部二次泰勒展开来近似该函数，然后直接找到这个二次近似函数的最小值点作为下一个迭代点 $x_{k+1}$。它需要用到目标函数的一阶导数（梯度 $g_k = \nabla F(x_k)$）和二阶导数（Hessian矩阵 $H_k = \nabla ^2F(x_k)$）。
- **推导 (直观理解)：**
  1. **泰勒展开：** 在点 $x_k$ 附近，$F(x)$ 近似为：
     $F(x_k + d) ≈ F(x_k) + g_k^T * d + (1/2) * d^T * H_k * d$
     其中 `d` 是搜索方向（从 $x_k$ 出发的位移向量）。
  2. **最小化二次模型：** 为了找到使这个二次近似函数最小的 $d$，我们对其关于 $d$ 求导并令导数为零：
     $\nabla _d [ ... ] = g_k + H_k * d = 0$
  3. **牛顿方程：** 得到线性方程组：
     $H_k * d = -g_k$
  4. **迭代更新：** 解这个线性方程组得到搜索方向 $d_k$，然后更新参数：
     $x_{k+1} = x_k + d_k$

### BFGS (Broyden–Fletcher–Goldfarb–Shanno)

- **核心思想**： BFGS 是最著名、最成功的拟牛顿法 (Quasi-Newton Method) 之一。它不需要显式计算 Hessian 矩阵 $H_k$。取而代之的是，它通过利用当前梯度和上一步的梯度信息，构造一个 Hessian 矩阵的逆 $B_k$（或其近似 $H_k^{-1}$）的近似矩阵。BFGS 直接维护并更新对 $H_k^{-1}$ 的近似 $B_k$。

- **关键特性:**

  - 避免显式 Hessian： 最大的优势是无需计算昂贵的 Hessian，只需计算目标函数的梯度 $g_k$。

  - 近似二阶信息： 通过分析连续迭代点处的梯度和位移变化 $(s_k = x_{k+1} - x_k, y_k = g_{k+1} - g_k)$，BFGS 巧妙地更新 $B_k$ 以满足割线条件 $B_{k+1} * y_k = s_k$。这个条件要求近似的 Hessian 逆在方向 $s_k$ 上的作用效果应与真实的 Hessian 逆一致（至少能正确预测梯度变化 $y_k$）。

  - 收敛速度： 虽然不如牛顿法快，但在适当的条件下（如 Wolfe 线搜索）通常具有超线性收敛速度（介于线性收敛和二次收敛之间），比仅用一阶梯度的方法（如梯度下降）快得多。

  - 保持正定性： BFGS 更新公式设计得当初始 $B_0$ 正定且满足曲率条件 $s_k^T y_k > 0$（通常由 Wolfe 线搜索保证）时，可以保证 $B_k$ 始终保持正定。这确保了搜索方向 $d_k = -B_k * g_k$ 总是下降方向。

  - 存储要求： 需要存储一个稠密的 $n \times n$ 矩阵 $B_k$。这对于大规模问题（$n$ 非常大）可能成为瓶颈（$O(n²)$ 存储）。针对此问题，有 L-BFGS (Limited-memory BFGS) 变种。

**L-BFGS**:

 - 解决存储问题： L-BFGS 是 BFGS 为处理大规模优化问题而设计的变体。它不存储完整的 $n \times n$ 近似矩阵 $B_k$。

 - 工作原理： 它只存储最近 $m$（通常 $m$ 很小，如 $5-20$）步的位移向量 $s_k$ 和梯度变化向量 $y_k$。利用这些向量，L-BFGS 在需要计算搜索方向 $d_k = -B_k * g_k$ 时，通过一个巧妙的递归公式，仅使用 $g_k$ 和存储的 ${s_i, y_i}$ 序列$（i = k-m, ..., k-1）$来隐式地计算出 $B_k * g_k$ 的结果（即 $-d_k$），而无需显式构造 $B_k$。

 - 优势： 存储需求从 $O(n²)$ 显著降低到 $O(m*n)$，使其能够高效处理参数维度 $n$ 高达数百万甚至更大的问题。收敛速度通常接近完整的 BFGS。

### 梯度下降法 (Gradient Descent) 及其变种：

- **基本思想：** 沿着当前点负梯度方向 (`d_k = -g_k`) 进行搜索，找到使函数值下降的点。是最简单的一阶方法。
- **变种：**
  - **最速下降法 (Steepest Descent)：** 一种特定的梯度下降，但在图形学中术语常混用。
  - **动量法 (Momentum)：** 在更新方向中加入上一步更新的惯性，帮助加速收敛并减少震荡（如穿越狭窄山谷）。
  - **Nesterov 加速梯度 (NAG)：** Momentum 的改进版，提供更好的理论收敛性质。
  - **AdaGrad/RMSProp/Adam：** **自适应学习率**算法。Adam (Adaptive Moment Estimation) 是目前**极其流行且鲁棒**的一阶优化器，它结合了动量（一阶矩估计）和自适应学习率（二阶矩估计），对超参数（尤其是学习率）不太敏感，在训练深度学习模型中几乎成为标配，在图形学中涉及深度学习的任务（如神经渲染、生成模型）也广泛应用。
